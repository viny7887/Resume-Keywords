{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5e6c691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'versatile', 'programming', 'language', 'used', 'data', 'analysis', 'web', 'development', 'AI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install PyPDF2\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Ensure you have the required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\"\n",
    "    Extract keywords (nouns and verbs) from the given text.\n",
    "    \"\"\"\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = pos_tag(filtered_words)\n",
    "    \n",
    "    # Extract nouns and verbs as keywords\n",
    "    keywords = [word for word, pos in pos_tags if pos.startswith('N') or pos.startswith('V')]\n",
    "    return keywords\n",
    "\n",
    "# Example usage\n",
    "text = \"Python is a versatile programming language used for data analysis, web development, and AI.\"\n",
    "keywords = extract_keywords(text)\n",
    "print(keywords)\n",
    "\n",
    "\n",
    "def process_resume(pdf_path):\n",
    "    \"\"\"\n",
    "    Processes the resume PDF and extracts keywords.\n",
    "    \"\"\"\n",
    "    # Extract text from the resume PDF\n",
    "    resume_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    if resume_text:\n",
    "        # Extract keywords from the resume text\n",
    "        keywords = extract_keywords(resume_text)\n",
    "        \n",
    "        if keywords:\n",
    "            print(f\"Keywords extracted from {pdf_path}:\")\n",
    "            for keyword in keywords:\n",
    "                print(f\"- {keyword}\")\n",
    "        else:\n",
    "            print(f\"No relevant keywords found in {pdf_path}.\")\n",
    "    else:\n",
    "        print(f\"Could not extract text from {pdf_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93845c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d160e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords extracted from C:\\Users\\Shree\\Downloads\\Resume_Demo_Atul.pdf:\n",
      "- Resume\n",
      "- Atul\n",
      "- Sharma\n",
      "- Email\n",
      "- atul.sharma\n",
      "- @\n",
      "- example.com\n",
      "- Phone\n",
      "- |\n",
      "- LinkedIn\n",
      "- Address\n",
      "- DEF\n",
      "- Street\n",
      "- Bengaluru\n",
      "- India\n",
      "- Objective\n",
      "- expertise\n",
      "- data\n",
      "- science\n",
      "- machine\n",
      "- learning\n",
      "- analytics\n",
      "- deliver\n",
      "- insights\n",
      "- organization\n",
      "- Skills\n",
      "- Python\n",
      "- Machine\n",
      "- Learning\n",
      "- Deep\n",
      "- Learning\n",
      "- Data\n",
      "- Analysis\n",
      "- SQL\n",
      "- Tableau\n",
      "- Language\n",
      "- Processing\n",
      "- Statistics\n",
      "- Work\n",
      "- Experience\n",
      "- Data\n",
      "- Scientist\n",
      "- Solutions\n",
      "- Pvt\n",
      "- Ltd.\n",
      "- March\n",
      "- Designed\n",
      "- models\n",
      "- increased\n",
      "- sales\n",
      "- %\n",
      "- Led\n",
      "- team\n",
      "- data\n",
      "- analysts\n",
      "- building\n",
      "- dashboards\n",
      "- monitor\n",
      "- business\n",
      "- KPIs\n",
      "- language\n",
      "- processing\n",
      "- models\n",
      "- customer\n",
      "- sentiment\n",
      "- analysis\n",
      "- Page\n",
      "- Resume\n",
      "- Data\n",
      "- Analyst\n",
      "- Innovate\n",
      "- Analytics\n",
      "- July\n",
      "- February\n",
      "- Analyzed\n",
      "- datasets\n",
      "- identify\n",
      "- trends\n",
      "- insights\n",
      "- business\n",
      "- strategies\n",
      "- Automated\n",
      "- data\n",
      "- pipelines\n",
      "- reducing\n",
      "- processing\n",
      "- time\n",
      "- %\n",
      "- Collaborated\n",
      "- stakeholders\n",
      "- deliver\n",
      "- recommendations\n",
      "- Education\n",
      "- Master\n",
      "- Science\n",
      "- Data\n",
      "- Science\n",
      "- IIT\n",
      "- Madras\n",
      "- Bachelor\n",
      "- Technology\n",
      "- Computer\n",
      "- Science\n",
      "- NIT\n",
      "- Trichy\n",
      "- Projects\n",
      "- Fraud\n",
      "- Detection\n",
      "- System\n",
      "- Built\n",
      "- machine\n",
      "- learning\n",
      "- model\n",
      "- transactions\n",
      "- %\n",
      "- accuracy\n",
      "- Segmentation\n",
      "- Used\n",
      "- clustering\n",
      "- segment\n",
      "- customers\n",
      "- targeted\n",
      "- marketing\n",
      "- campaigns\n",
      "- Stock\n",
      "- Price\n",
      "- Prediction\n",
      "- Developed\n",
      "- deep\n",
      "- learning\n",
      "- model\n",
      "- forecast\n",
      "- stock\n",
      "- prices\n",
      "- using\n",
      "- LSTMs\n",
      "- Page\n"
     ]
    }
   ],
   "source": [
    "# Provide the path to your resume PDF\n",
    "pdf_path = r\"C:\\Users\\Shree\\Downloads\\Resume_Demo_Atul.pdf\"\n",
    "\n",
    "# Call the function to process the resume and extract keywords\n",
    "process_resume(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803f7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1562bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
